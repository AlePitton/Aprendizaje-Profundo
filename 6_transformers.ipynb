{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers para Martin Fierro\n",
    "\n",
    "Los dos principales objetivos de este tutorial son mostrar c√≥mo es una arquitectura transformer y c√≥mo usar las herramientas provistas por [Hugging Face](https://huggingface.co/). En este tutorial usaremos una implementaci√≥n de [GPT2](https://openai.com/blog/better-language-models/) en espa√±ol para generar (una vez m√°s) texto similar al Mart√≠n Fierro, que se puede descargar [aqu√≠](https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt).\n",
    "\n",
    "Cr√©ditos a [Jay Alammar](https://jalammar.github.io/illustrated-gpt2/) por las im√°genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import wget\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import unicodedata, re\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "\n",
    "GPT2 es un modelo de generaci√≥n de texto basado en transformers. A diferencia de BERT u otros transformers, GPT2 est√° basado en sucesivas instancias de decoders\n",
    "\n",
    "<img src=\"images/gpt-2-transformer-xl-bert-3.png\"\n",
    "     alt=\"GPT2 Decoder Architecture\"\n",
    "     style=\"float: center; margin-right: 150px;\"\n",
    "     width=75%/>\n",
    "     \n",
    "<img src=\"images/gpt2_function.gif\"\n",
    "     alt=\"GPT2 Text Generation Function\"\n",
    "     style=\"float: center; margin-right: 150px;\"\n",
    "     width=75%/>\n",
    "     \n",
    "**Recursos:**\n",
    "\n",
    "[The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Usando transformers de Hugging Face\n",
    "\n",
    "La librer√≠a transformers permite usar modelos ya entrenados en un pipeline y nos facilita funciones que procesan el input, lo tokenizan y hace el forward pass para generar las predicciones.\n",
    "\n",
    "Dada una tarea, pipeline descarga un modelo y tokenizador apropiados para la tarea. En este caso, especificamos un modelo preentrenado de GPT2-small en idioma espa√±ol.\n",
    "\n",
    "<img src=\"images/gpt2_sizes.png\"\n",
    "     alt=\"GPT2 Model Sizes\"\n",
    "     style=\"float: center; margin-right: 200px;\"\n",
    "     width=70%/>\n",
    "     \n",
    "<img src=\"images/gpt2_decoder_stacks.png\"\n",
    "     alt=\"GPT2 Model Sizes\"\n",
    "     style=\"float: center; margin-right: 200px;\"\n",
    "     width=70%/>\n",
    "     \n",
    "\n",
    "Para nuestra tarea en particular, pipeline tomar√° un texto e internamente se encargar√° del tokenizado de la oraci√≥n y de generar texto hasta el *max_length* indicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at datificate/gpt2-small-spanish.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at datificate/gpt2-small-spanish.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model=\"datificate/gpt2-small-spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Ah√≠ dice: \"los miminos bien\"\n",
      "...   y el de \"los tritones son muy dolorosos\".\n",
      "\n",
      "\n",
      "El rey Arturo, hijo de la princesa Olvi (Mar√≠a de Molina), se opone al matrimonio: √©l quiere que nadie los consuma como reina, pues considera que lo que la ha hecho feliz, para √©l, es el matrimonio con una mujer en la forma de mujer. Arturo, a su vez, no ha sido un candidato para alcanzar a la corona, pero se\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'Ah√≠ dice: \"los miminos bien\"'\n",
    "output = generator(input_sentence, max_length=100)\n",
    "print('Generated Text: %s' % input_sentence)\n",
    "print('... ', output[0]['generated_text'][len(input_sentence):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar otras tareas de ü§ó Transformers, [aqu√≠](https://huggingface.co/transformers/task_summary.html) hay una lista completa de todas las tareas que se implementan.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Importando de ü§ó Transformers\n",
    "\n",
    "Lo primero es importar el modelo con el cu√°l trabajaremos. En nuestro caso usaremos [GPT2-Small en espa√±ol de Datificate](https://huggingface.co/datificate/gpt2-small-spanish) (pueden jugar con el modelo desde la misma p√°gina de ü§ó). \n",
    "\n",
    "Nuestrea tarea de inter√©s es hacer Language Modeling del Mart√≠n Fierro, por lo cual importamos **GPT2LMHeadModel**, que es una implementaci√≥n de GPT2 con la √∫ltima capa lineal para retornar los logits del tama√±o del vocabulario.\n",
    "\n",
    "Como el modelo *datificate/gpt2-small-spanish* fu√© entrenado en tensorflow, usamos el flag *from_tf=True* para asegurarnos que importe los pesos correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "Some weights of GPT2LMHeadModel were not initialized from the TF 2.0 model and are newly initialized: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('datificate/gpt2-small-spanish', from_tf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso siguiente es descargar un tokenizador. El tokenizador har√° por nosotres el trabajo de separar el texto en palabras y convertirlas en sus correspondientes ids dentro del vocabulario. Al igual que con el modelo, ü§ó provee tokenizers ya implementados para sus correspondientes modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_ids: [4701, 2686, 498, 363, 27422, 6242, 258, 599, 10007, 15308, 657]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tokens: ['ƒ†Yo', 'ƒ†nunca', 'ƒ†hab', 'ia', 'ƒ†escuchado', 'ƒ†hablar', 'ƒ†de', 'ƒ†In', 'ns', 'mouth', 'ƒ†hasta']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('datificate/gpt2-small-spanish', add_prefix_space=True)\n",
    "\n",
    "tokenized_seq = tokenizer(\"Yo nunca habia escuchado hablar de Innsmouth hasta\")\n",
    "print('inputs_ids: {}'.format(tokenized_seq['input_ids']))\n",
    "print('attention_mask: {}'.format(tokenized_seq['attention_mask']))\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_seq['input_ids'])\n",
    "print('tokens: {}'.format(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tokenizer se puede encargar por nosotres de manejar temas de padding y truncado entre otros. Para m√°s info en las funciones del tokenizers, mirar [aqu√≠](https://huggingface.co/transformers/main_classes/tokenizer.html).\n",
    "\n",
    "El tokenizer tambi√©n puede procesar m√∫ltiples oraciones al mismo tiempo y retornar tensores para el framework que estemos usando de base, sea este TensorFlow 2.0 ('*tf*') o PyTorch ('*pt*')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Yo nunca habia escuchado hablar de Innsmouth hasta\",\n",
    "             \"Hacia el oeste de Arkham las monta√±as se levantaban ind√≥mitas y en sus entra√±as\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_ids:\n",
      "tensor([[ 4701,  2686,   498,   363, 27422,  6242,   258,   599, 10007, 15308,\n",
      "           657,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [11945,   284,  2866,   258, 18307,  2747,   347,  6674,   306,  3620,\n",
      "          4413,  1582,  1178,  2982,   287,   278,   452,  7259,  1846]])\n",
      "attention_mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_seq = tokenizer(sentences,\n",
    "                          padding=True,\n",
    "                          return_tensors=\"pt\")\n",
    "\n",
    "print('inputs_ids:\\n{}'.format(tokenized_seq['input_ids']))\n",
    "print('attention_mask:\\n{}'.format(tokenized_seq['attention_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluso el tokenizer puede manejar listas de palabras con solo agregar el par√°metro *is_split_into_words=True*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_ids:\n",
      "tensor([[ 4701,  2686,   498,   363, 27422,  6242,   258,   599, 10007, 15308,\n",
      "           657,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [11945,   284,  2866,   258, 18307,  2747,   347,  6674,   306,  3620,\n",
      "          4413,  1582,  1178,  2982,   287,   278,   452,  7259,  1846]])\n",
      "attention_mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_seq = tokenizer([sent.split() for sent in sentences],\n",
    "                          padding=True,\n",
    "                          return_tensors=\"pt\",\n",
    "                          is_split_into_words=True)\n",
    "\n",
    "print('inputs_ids:\\n{}'.format(tokenized_seq['input_ids']))\n",
    "print('attention_mask:\\n{}'.format(tokenized_seq['attention_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 2: Finetune con PyTorch nativo\n",
    "\n",
    "Para entrenar con PyTorch, necesitamos crear una instancia de un Dataset de PyTorch. Esta vez, podemos usar el tokenizer que ya importamos para hacer el encoding de las palabras.\n",
    "\n",
    "Por simplicidad, dividiremos el el conjunto de datos en palabras y usaremos listas de palabras de tama√±o fijo.\n",
    "\n",
    "Para entrenamiento nativo en Tensorflow 2, ver [aqu√≠](https://huggingface.co/transformers/training.html#fine-tuning-in-native-tensorflow-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MartinFierroDatasetGPT2(Dataset):\n",
    "    def __init__(self, textdata, maxlen, tokenizer):\n",
    "        \n",
    "        self.maxlen = maxlen\n",
    "        # Get ourselves a list of words so we can iterate\n",
    "        split_text = textdata.split()\n",
    "        # cut the text in sequences of maxlen characters\n",
    "        self.sentences = {}\n",
    "        for idx, i in enumerate(range(0, len(split_text) - maxlen - 1, maxlen)):\n",
    "            self.sentences[idx] = split_text[i: i + maxlen]\n",
    "\n",
    "        # You need to activate padding in order to\n",
    "        # return tensors\n",
    "        self.data = tokenizer(list(self.sentences.values()),\n",
    "                              padding=True,\n",
    "                              is_split_into_words=True,\n",
    "                              return_tensors=\"pt\")\n",
    "        \n",
    "        self.length = len(self.sentences)\n",
    "        print('NB sequences:', self.length)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 33858\n",
      "NB sequences: 128\n"
     ]
    }
   ],
   "source": [
    "with open('./martin_fierro.txt', 'r') as finput:\n",
    "    text = unicodedata.normalize('NFC', finput.read()).lower()\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "\n",
    "print('Corpus length: %d' % len(text))\n",
    "\n",
    "train_dataset = MartinFierroDatasetGPT2(text, 50, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "device = torch.device('cuda') if use_cuda else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "loss_function = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_config = {'dataset': train_dataset,\n",
    "                     'batch_size': 1,\n",
    "                     'shuffle': True,\n",
    "                     'num_workers': 0,\n",
    "                     'pin_memory': use_cuda}\n",
    "\n",
    "dataloader = DataLoader(**dataloader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "stream = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "for i, sample in stream:\n",
    "    start = time()\n",
    "    input_ids = sample['input_ids'].to(device)\n",
    "    attention_mask = sample['attention_mask'].to(device)\n",
    "    \n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    stream.set_description('loss=%g, elapsed=%g' % (loss, time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Entrenando con Transformers Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "                                                mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./gpt2-fierro',      # output directory\n",
    "    num_train_epochs=1,              # total # of training epochs\n",
    "    per_device_train_batch_size=1 ,  # batch size per device during training\n",
    "    per_device_eval_batch_size=1,    # batch size for evaluation\n",
    "    warmup_steps=1,                  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                     # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,              # training arguments, defined above\n",
    "    train_dataset=train_dataset,     # training dataset (THE SAME AS BEFORE)\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/mmazuecos/anaconda3/envs/visdial/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/users/mmazuecos/anaconda3/envs/visdial/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='43' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43/43 00:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=43, training_loss=5.386078945426053, metrics={'train_runtime': 29.5643, 'train_samples_per_second': 1.454, 'total_flos': 8314570211328, 'epoch': 1.0})"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Usando nuestro nuevo modelo en un pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
