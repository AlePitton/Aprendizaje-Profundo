{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers para Martin Fierro\n",
    "\n",
    "Los dos principales objetivos de este tutorial son mostrar c√≥mo es una arquitectura transformer y c√≥mo usar las herramientas provistas por [Hugging Face](https://huggingface.co/). En este tutorial usaremos una implementaci√≥n de [GPT2](https://openai.com/blog/better-language-models/) en espa√±ol para generar (una vez m√°s) texto similar al Mart√≠n Fierro, que se puede descargar [aqu√≠](https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt).\n",
    "\n",
    "Cr√©ditos a [Jay Alammar](https://jalammar.github.io/illustrated-gpt2/) por las im√°genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import wget\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import unicodedata, re\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "\n",
    "GPT2 es un modelo de generaci√≥n de texto basado en transformers. A diferencia de BERT u otros transformers, GPT2 est√° basado en sucesivas instancias de decoders\n",
    "\n",
    "<img src=\"images/gpt-2-transformer-xl-bert-3.png\"\n",
    "     alt=\"GPT2 Decoder Architecture\"\n",
    "     style=\"float: center; margin-right: 150px;\"\n",
    "     width=75%/>\n",
    "     \n",
    "<img src=\"images/gpt2_function.gif\"\n",
    "     alt=\"GPT2 Text Generation Function\"\n",
    "     style=\"float: center; margin-right: 150px;\"\n",
    "     width=75%/>\n",
    "     \n",
    "**Recursos:**\n",
    "\n",
    "[The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Usando transformers de Hugging Face\n",
    "\n",
    "La librer√≠a transformers permite usar modelos ya entrenados en un pipeline y nos facilita funciones que procesan el input, lo tokenizan y hace el forward pass para generar las predicciones.\n",
    "\n",
    "Dada una tarea, pipeline descarga un modelo y tokenizador apropiados para la tarea. En este caso, especificamos un modelo preentrenado de GPT2-small en idioma espa√±ol.\n",
    "\n",
    "<img src=\"images/gpt2_sizes.png\"\n",
    "     alt=\"GPT2 Model Sizes\"\n",
    "     style=\"float: center; margin-right: 200px;\"\n",
    "     width=70%/>\n",
    "     \n",
    "<img src=\"images/gpt2_decoder_stacks.png\"\n",
    "     alt=\"GPT2 Model Sizes\"\n",
    "     style=\"float: center; margin-right: 200px;\"\n",
    "     width=70%/>\n",
    "     \n",
    "\n",
    "Para nuestra tarea en particular, pipeline tomar√° un texto e internamente se encargar√° del tokenizado de la oraci√≥n y de generar texto hasta el *max_length* indicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at datificate/gpt2-small-spanish.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at datificate/gpt2-small-spanish.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model=\"datificate/gpt2-small-spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Yo he visto en esa milonga muchos Gefes con estancia,...\n",
      "...   se lo puede imaginar y me han gustado algunos. \n",
      "El \"Rule in London\" de The Beatles y, aunque en ese grupo no fue muy popular ya que su\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'Yo he visto en esa milonga muchos Gefes con estancia,'\n",
    "output = generator(input_sentence, max_length=50)\n",
    "print('Generated Text: %s...' % input_sentence)\n",
    "print('... ', output[0]['generated_text'][len(input_sentence):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar otras tareas de ü§ó Transformers, [aqu√≠](https://huggingface.co/transformers/task_summary.html) hay una lista completa de todas las tareas que se implementan.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Importando de ü§ó Transformers\n",
    "\n",
    "Lo primero es importar el modelo con el cu√°l trabajaremos. En nuestro caso usaremos [GPT2-Small en espa√±ol de Datificate](https://huggingface.co/datificate/gpt2-small-spanish) (pueden jugar con el modelo desde la misma p√°gina de ü§ó). \n",
    "\n",
    "Nuestrea tarea de inter√©s es hacer Language Modeling del Mart√≠n Fierro, por lo cual importamos **GPT2LMHeadModel**, que es una implementaci√≥n de GPT2 con la √∫ltima capa lineal para retornar los logits del tama√±o del vocabulario.\n",
    "\n",
    "Como el modelo *datificate/gpt2-small-spanish* fu√© entrenado en tensorflow, usamos el flag *from_tf=True* para asegurarnos que importe los pesos correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "Some weights of GPT2LMHeadModel were not initialized from the TF 2.0 model and are newly initialized: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('datificate/gpt2-small-spanish', from_tf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso siguiente es descargar un tokenizador. El tokenizador har√° por nosotres el trabajo de separar el texto en palabras y convertirlas en sus correspondientes ids dentro del vocabulario. Al igual que con el modelo, ü§ó provee tokenizers ya implementados para sus correspondientes modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_ids: [4701, 2686, 498, 363, 27422, 6242, 258, 599, 10007, 15308, 657]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tokens: ['ƒ†Yo', 'ƒ†nunca', 'ƒ†hab', 'ia', 'ƒ†escuchado', 'ƒ†hablar', 'ƒ†de', 'ƒ†In', 'ns', 'mouth', 'ƒ†hasta']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('datificate/gpt2-small-spanish', add_prefix_space=True)\n",
    "\n",
    "tokenized_seq = tokenizer(\"Yo nunca habia escuchado hablar de Innsmouth hasta\")\n",
    "print('inputs_ids: {}'.format(tokenized_seq['input_ids']))\n",
    "print('attention_mask: {}'.format(tokenized_seq['attention_mask']))\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_seq['input_ids'])\n",
    "print('tokens: {}'.format(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tokenizer se puede encargar por nosotres de manejar temas de padding y truncado entre otros. Para m√°s info en las funciones del tokenizers, mirar [aqu√≠](https://huggingface.co/transformers/main_classes/tokenizer.html).\n",
    "\n",
    "El tokenizer tambi√©n puede procesar m√∫ltiples oraciones al mismo tiempo y retornar tensores para el framework que estemos usando de base, sea este TensorFlow 2.0 ('*tf*') o PyTorch ('*pt*')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Yo nunca habia escuchado hablar de Innsmouth hasta\",\n",
    "             \"Hacia el oeste de Arkham las monta√±as se levantaban ind√≥mitas y en sus entra√±as\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_ids:\n",
      "tensor([[ 4701,  2686,   498,   363, 27422,  6242,   258,   599, 10007, 15308,\n",
      "           657,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [11945,   284,  2866,   258, 18307,  2747,   347,  6674,   306,  3620,\n",
      "          4413,  1582,  1178,  2982,   287,   278,   452,  7259,  1846]])\n",
      "attention_mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_seq = tokenizer(sentences,\n",
    "                          padding=True,\n",
    "                          return_tensors=\"pt\")\n",
    "\n",
    "print('inputs_ids:\\n{}'.format(tokenized_seq['input_ids']))\n",
    "print('attention_mask:\\n{}'.format(tokenized_seq['attention_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluso el tokenizer puede manejar listas de palabras con solo agregar el par√°metro *is_split_into_words=True*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_ids:\n",
      "tensor([[ 4701,  2686,   498,   363, 27422,  6242,   258,   599, 10007, 15308,\n",
      "           657,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [11945,   284,  2866,   258, 18307,  2747,   347,  6674,   306,  3620,\n",
      "          4413,  1582,  1178,  2982,   287,   278,   452,  7259,  1846]])\n",
      "attention_mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_seq = tokenizer([sent.split() for sent in sentences],\n",
    "                          padding=True,\n",
    "                          return_tensors=\"pt\",\n",
    "                          is_split_into_words=True)\n",
    "\n",
    "print('inputs_ids:\\n{}'.format(tokenized_seq['input_ids']))\n",
    "print('attention_mask:\\n{}'.format(tokenized_seq['attention_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 2: Finetune con PyTorch nativo\n",
    "\n",
    "Para entrenar con PyTorch, necesitamos crear una instancia de un Dataset de PyTorch. Esta vez, podemos usar el tokenizer que ya importamos para hacer el encoding de las palabras.\n",
    "\n",
    "Por simplicidad, dividiremos el el conjunto de datos en palabras y usaremos listas de palabras de tama√±o fijo.\n",
    "\n",
    "Para entrenamiento nativo en Tensorflow 2, ver [aqu√≠](https://huggingface.co/transformers/training.html#fine-tuning-in-native-tensorflow-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MartinFierroDatasetGPT2(Dataset):\n",
    "    def __init__(self, textdata, maxlen, tokenizer):\n",
    "        \n",
    "        self.maxlen = maxlen\n",
    "        # Get ourselves a list of words so we can iterate\n",
    "        split_text = textdata.split()\n",
    "        # cut the text in sequences of maxlen characters\n",
    "        self.sentences = {}\n",
    "        for idx, i in enumerate(range(0, len(split_text) - maxlen - 1, maxlen)):\n",
    "            self.sentences[idx] = split_text[i: i + maxlen]\n",
    "\n",
    "        # You need to activate padding in order to\n",
    "        # return tensors\n",
    "        self.data = tokenizer(list(self.sentences.values()),\n",
    "                              padding=True,\n",
    "                              is_split_into_words=True,\n",
    "                              return_tensors=\"pt\")\n",
    "        \n",
    "        self.length = len(self.sentences)\n",
    "        print('NB sequences:', self.length)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el archivo e instanciamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 33858\n",
      "NB sequences: 128\n"
     ]
    }
   ],
   "source": [
    "with open('./martin_fierro.txt', 'r') as finput:\n",
    "    text = unicodedata.normalize('NFC', finput.read()).lower()\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "\n",
    "print('Corpus length: %d' % len(text))\n",
    "\n",
    "train_dataset = MartinFierroDatasetGPT2(text, 50, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponemos nuestro modelo en modo train y seteamos el dispositivo donde vamos a ejecutar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "device = torch.device('cuda') if use_cuda else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos de transformers la implementacion de [AdamW](https://arxiv.org/abs/1711.05101)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos nuestro dataloader con el dataset del Mart√≠n Fierro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_config = {'dataset': train_dataset,\n",
    "                     'batch_size': 8,\n",
    "                     'shuffle': True,\n",
    "                     'num_workers': 0,\n",
    "                     'pin_memory': use_cuda}\n",
    "\n",
    "dataloader = DataLoader(**dataloader_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente hacemos un forward pass por todo el dataset (i.e. entrenamos por una epoch). El modelo importado de ü§ó transformers ya se encarga de calcular la loss para la tarea que necesitamos (en este caso es un LM) en el forward pass si le pasamos el argumento *labels*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb6b906e8cf47f490af9df0d50e0937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/mmazuecos/anaconda3/envs/visdial/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "stream = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "for i, sample in stream:\n",
    "    input_ids = sample['input_ids'].to(device)\n",
    "    attention_mask = sample['attention_mask'].to(device)\n",
    "    \n",
    "    outputs = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    stream.set_postfix({'loss': loss.detach().cpu().numpy()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Entrenando con Transformers Trainers\n",
    "\n",
    "ü§ó tambi√©n provee sus propios m√©todos para entrenar. En este caso:\n",
    "  - TrainingArgument: nos genera una configuraci√≥n para un entrenamiento\n",
    "  - Trainer: clase que se encargar√° del entrenamiento por nosotres\n",
    "  \n",
    "Importamos el Data Collator necesario para nuestra tarea y pasamos los argumentos necesario para cada objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# We're not training with MLM, so we must set mlm=False\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "                                                mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./gpt2-fierro',      # output directory\n",
    "    num_train_epochs=1,              # total # of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    per_device_eval_batch_size=1,    # batch size for evaluation\n",
    "    warmup_steps=1,                  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                     # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,              # training arguments, defined above\n",
    "    train_dataset=train_dataset,     # training dataset (THE SAME AS BEFORE)\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, entrenar es tan simple como llamar al m√©todo train() del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/mmazuecos/anaconda3/envs/visdial/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 01:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16, training_loss=5.797689437866211, metrics={'train_runtime': 111.7562, 'train_samples_per_second': 0.143, 'total_flos': 8314570211328, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 4: Usando nuestro nuevo modelo en un pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Yo he visto en esa milonga muchos Gefes con estancia,...\n",
      "...   y las dos est√°n bien, y con todas sus alegr√≠as, tan se me voy a llevarnos un d√≠a\". \"He aqu√≠ qu√© de lo que soy su perdici√≥n para\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'Yo he visto en esa milonga muchos Gefes con estancia,'\n",
    "output = generator(input_sentence, max_length=50)\n",
    "print('Generated Text: %s...' % input_sentence)\n",
    "print('... ', output[0]['generated_text'][len(input_sentence):])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
