{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0b02df97-cbb3-4759-9371-cbbecd0ccd86"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Construyendo una red neuronal con Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "03d05899-7ff9-4413-aa67-c7a96bbdfcde"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ¿Qué librerías necesitamos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fa44eec5-93b3-4e4b-adcb-1065bb5cc474"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Otras librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "79849185-892a-41ce-b4a2-26db6ad19597"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cargando los datos del MNIST\n",
    "\n",
    "- El conjunto de datos a utilizar es el **[MNIST](http://yann.lecun.com/exdb/mnist/)**.\n",
    "- Es un conjunto estándar para hacer *reconocimiento de imágenes*.\n",
    "- Buscamos entrenar un clasificador que reconozca que dígito es mostrado en la imagen.\n",
    "- El MNIST está compuesto por imágenes de 28x28 píxeles representadas como matrices.\n",
    "- La salida son 10 clases (dígitos del 0 al 9).\n",
    "- Se preprocesará para *convertir las matrices en vectores* y transformar las etiquetas en representaciones *one-of-k*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # For mini-batch gradient descent\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "input_size = 28*28\n",
    "train_examples = 60000\n",
    "test_examples = 10000\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape the dataset to convert the examples from 2D matrixes to 1D arrays.\n",
    "x_train = x_train.reshape(train_examples, input_size)\n",
    "x_test = x_test.reshape(test_examples, input_size)\n",
    "\n",
    "# normalize the input\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contruyendo la red neuronal\n",
    "\n",
    "- Comenzaremos por construir un *perceptrón multicapa* que es la red neuronal más común.\n",
    "- El modelo más simple en Keras es una concatenación de capas (layers).\n",
    "    - Se llama modelo secuencial.\n",
    "- La capa más básica es la *densa* (dense o fully connected).\n",
    "    - Internamente tiene dos variables: una matriz de pesos y un vector de biases. Keras nos abstrae de todo eso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Input to hidden layer\n",
    "model.add(Dense(512, input_shape=(input_size,)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Hidden to output layer\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para imprimir una descripción del modelo existe un comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Funciones de Activación\n",
    "\n",
    "- Una red neuronal con activación lineal no tiene mucho más poder de representación que un algoritmo lineal.\n",
    "- Para expresar no linearidad en la red neuronal se necesitan funciones no lineales de activación.\n",
    "- Una función de activación común es la *sigmoide* (o logística).\n",
    "- Keras soporta varias funciones de activación: rectified linea unit (ReLU), tangenge hiperbólica, sigmoide \"dura\", etc.\n",
    "    - Hoy en día, por sus propiedades, ReLU suele ser la más utilizada [1].\n",
    "- La función de activación *softmax* es utilizada al final de la red y sirve para clasificación.\n",
    "\n",
    "![Funciones de activación](images/activation_functions.png \"Funciones de activación\")\n",
    "<div style=\"text-align: right;\">Fuente: https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 52,650\n",
      "Trainable params: 52,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(784,), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularización de la red\n",
    "\n",
    "### Regularización de los pesos\n",
    "\n",
    "- La red puede regularizarse penalizando los pesos.\n",
    "- Los pesos se regularizan mediante alguna norma:\n",
    "    - L1 es la suma del valor absoluto: ${\\displaystyle \\lambda \\sum_{i=1}^{k} |w_i|}$\n",
    "    - L2 es la suma del valor cuadrado, es la más común: ${\\displaystyle \\lambda \\sum_{i=1}^{k} w_i^2}$\n",
    "    - Elastic net es una combinación de ambas: ${\\displaystyle \\lambda_1 \\sum_{i=1}^{k} |w_i| + \\lambda_2 \\sum_{i=1}^{k} w_i^2}$\n",
    "- Para un análisis detallado de la diferencia entre L1 y L2 revisar [\\[2\\]](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(784,), activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(10, activation='softmax', kernel_regularizer=regularizers.l2(0.01))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dropout\n",
    "\n",
    "- Otra forma muy usada a la hora de regularizar es el **dropout** [3].\n",
    "- Es extremadamente efectivo y simple.\n",
    "- Es complementario a L1/L2/ElasticNet.\n",
    "- Durante el entrenamiento se implementa apagando un neurón con alguna probabilidad **_p_** (un hiperparámetro).\n",
    "\n",
    "![Dropout](images/dropout.jpeg \"Dropout\")\n",
    "<div style=\"text-align: right;\">Fuente: Trabajo de Srivastava et al. [3]</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dropout en Keras\n",
    "\n",
    "- Se aplica agregando capas al modelo.\n",
    "- Se llaman capas `Dropout` y se agrega a cada capa que se quiere regularizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_shape=(784,), activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparando el modelo para entrenarlo\n",
    "\n",
    "- Para minimizar una red neuronal necesitamos *calcular sus gradientes*.\n",
    "    - Esto se hace con el algoritmo de *retropropagación*.\n",
    "- Keras tiene la capacidad de hacerlo automáticamente.\n",
    "    - Esto se conoce como _diferenciación automática_ y es algo común en los frameworks de deep learning.\n",
    "- El modelo de Keras necesita *compilarse*.\n",
    "    - Recordar que lo que armamos en Keras (o TensorFlow) es un grafo de computación.\n",
    "- Al compilar el modelo los parámetros más importantes son la función de costo y el algoritmo de optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Funciones de costo y algoritmos de optimización\n",
    "\n",
    "- La función de costo puede cambiar de acuerdo al tipo de problema (clasificación binaria/multiclase o regresión).\n",
    "    - La funciones más comunes son la media del error cuadrático (_mean squared error_) para regresión y la entropía cruzada (_crossentropy_) para clasificación.\n",
    "- El algoritmo de optimización es el que entrena la red. Existen varios, que en si son variaciones del algoritmo de _descenso por la gradiente_.\n",
    "\n",
    "<div style=\"text-align: center; margin: 5px 0;\">\n",
    "    <div style=\"display: inline-block;\">\n",
    "        <img src=\"images/contours_evaluation_optimizers.gif\" alt=\"Optimización\" style=\"width: 350px;\"/>\n",
    "    </div>\n",
    "    <div style=\"display: inline-block;\">\n",
    "        <img src=\"images/saddle_point_evaluation_optimizers.gif\" alt=\"Optimización\" style=\"width: 350px;\"/>\n",
    "    </div>\n",
    "</div>\n",
    "<div style=\"text-align: right;\">Fuente: http://ruder.io/optimizing-gradient-descent/</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Compilando el modelo Keras y visualizando la arquitectura\n",
    "\n",
    "- Con el método `.compile()` podemos compilar el modelo de Keras.\n",
    "- Además de la función de costo y el algoritmo de optimización se le pueden pasar métricas para llevar registro además del error de los datos (e.g. la exactitud o la precisión).\n",
    "- Una vez compilado el modelo la modificación requerirá rehacerlo desde cero (salvo que se guarden y carguen los pesos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adagrad(lr=0.001, decay=0.0001), \n",
    "                  # También podría ser el string \"Adagrad\" con los parámetros por defecto\n",
    "              metrics=['accuracy'])  # La métrica sirve para llevar algún registro además del costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualizar la arquitectura (opcional)\n",
    "\n",
    "Opcionalmente, si instalamos las librerías extras pedidas en el setup y utilizando `vis_util`, podemos visualizar el grafo de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 133.00 264.00\" width=\"133pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-260 129,-260 129,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139687585687032 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139687585687032</title>\n",
       "<polygon fill=\"none\" points=\"11.5,-146.5 11.5,-182.5 113.5,-182.5 113.5,-146.5 11.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-160.8\">dense_9: Dense</text>\n",
       "</g>\n",
       "<!-- 139687582557184 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139687582557184</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 125,-109.5 125,-73.5 0,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-87.8\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 139687585687032&#45;&gt;139687582557184 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139687585687032-&gt;139687582557184</title>\n",
       "<path d=\"M62.5,-146.4551C62.5,-138.3828 62.5,-128.6764 62.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"66.0001,-119.5903 62.5,-109.5904 59.0001,-119.5904 66.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139687582556680 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139687582556680</title>\n",
       "<polygon fill=\"none\" points=\"8,-.5 8,-36.5 117,-36.5 117,-.5 8,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-14.8\">dense_10: Dense</text>\n",
       "</g>\n",
       "<!-- 139687582557184&#45;&gt;139687582556680 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139687582557184-&gt;139687582556680</title>\n",
       "<path d=\"M62.5,-73.4551C62.5,-65.3828 62.5,-55.6764 62.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"66.0001,-46.5903 62.5,-36.5904 59.0001,-46.5904 66.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139687582445296 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139687582445296</title>\n",
       "<polygon fill=\"none\" points=\"3.5,-219.5 3.5,-255.5 121.5,-255.5 121.5,-219.5 3.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-233.8\">139687582445296</text>\n",
       "</g>\n",
       "<!-- 139687582445296&#45;&gt;139687585687032 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139687582445296-&gt;139687585687032</title>\n",
       "<path d=\"M62.5,-219.4551C62.5,-211.3828 62.5,-201.6764 62.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"66.0001,-192.5903 62.5,-182.5904 59.0001,-192.5904 66.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entrenamiento\n",
    "\n",
    "- Una vez compilado el modelo, está listo para ser entrenado.\n",
    "- Keras tiene una interfaz similar a Scikit-Learn, con lo método `fit` y `predict`.\n",
    "- Para entrenar se necesitan 3 parámetros:\n",
    "    - El conjunto de datos de entrenamiento (datos y etiquetas).\n",
    "    - El tamaño del batch para hacer _mini-batch gradient descent_.\n",
    "    - La cantidad de épocas que entrenar.\n",
    "    - Eventualmente le podemos pasar datos para hacer validación del modelo.\n",
    "    - El parámetro `verbose` nos imprime información útil respecto al desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 2.1583 - acc: 0.5800 - val_loss: 1.5160 - val_acc: 0.8273\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 1.5577 - acc: 0.7176 - val_loss: 1.2365 - val_acc: 0.8572\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 1.3647 - acc: 0.7539 - val_loss: 1.1047 - val_acc: 0.8679\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 1.2643 - acc: 0.7702 - val_loss: 1.0272 - val_acc: 0.8748\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 1.1947 - acc: 0.7829 - val_loss: 0.9726 - val_acc: 0.8777\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 1.1484 - acc: 0.7901 - val_loss: 0.9332 - val_acc: 0.8817\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 1.1078 - acc: 0.7952 - val_loss: 0.9024 - val_acc: 0.8837\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 1.0821 - acc: 0.8006 - val_loss: 0.8779 - val_acc: 0.8842\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 1.0576 - acc: 0.8049 - val_loss: 0.8572 - val_acc: 0.8871\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 1.0385 - acc: 0.8092 - val_loss: 0.8396 - val_acc: 0.8890\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          batch_size=batch_size, epochs=epochs, \n",
    "          validation_data=(x_test, y_test), verbose=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Referencias\n",
    "\n",
    "- [1] LeCun, Yann, Bengio, Yoshua, and Hinton, Geoffrey. \"Deep learning.\" Nature 521, no. 7553 (2015): 436-444.\n",
    "- [2] \"Differences between L1 and L2 as Loss Function and Regularization\". http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/\n",
    "- [3] Srivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. \"Dropout: a simple way to prevent neural networks from overfitting.\" Journal of machine learning research 15, no. 1 (2014): 1929-1958. Harvard.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:diplodatos]",
   "language": "python",
   "name": "conda-env-diplodatos-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
