{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs para Martín Fierro\n",
    "\n",
    "El objetivo de los ejercicios en este tutorial es mostrar el impacto de algunas decisiones de diseño en la implementación de las redes neuronales, particularmente las recurrentes. Como ejemplo veremos una implementación de la red RNN para generación de lenguaje basada en caracteres de [Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Para entrenarla utilizaremos un fragmento del Martín Fierro que pueden descargar [aquí](https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt). Para un entrenamiento más complejo, pueden utilizar las obras completas de borges, disponibles en [este link](https://drive.google.com/file/d/0B4remi0ZCiqbUFpTS19pSmVFYkU/view?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import wget\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero leeremos el dataset del archivo de texto y lo preprocesaremos para disminuir la viariación de caracteres. Normalizaremos el formato unicos, elminaremos espacios y transformaremos todo a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('martin_fierro.txt'):\n",
    "    wget.download('https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 33858\n"
     ]
    }
   ],
   "source": [
    "with open('./martin_fierro.txt', 'r') as finput:\n",
    "    text = unicodedata.normalize('NFC', finput.read()).lower()\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "\n",
    "print('Corpus length: %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, contaremos la cantidad de caracteres únicos presentes en el texto, y le asignaremos a cada uno un índice único y secuencial. Este índice será utilizado luego para crear las representaciones one-hot encoding de los caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 54\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "\n",
    "print('Total chars: %d' % len(chars))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Esqueleto de la red neuronal\n",
    "\n",
    "Lo primero que debemos pensar es cómo será la arquitectura de nuestra red para resolver la tarea deseada. En esta sección crearemos el modelo sequencial con PyTorch que representará nuestra red. En los pasos siguientes, implementaremos las transformaciones del corpus, por lo que en este paso pueden asumir cualquier formato en los datos de entrada.\n",
    "\n",
    "Para poder implementar el modelo debemos responder las siguientes preguntas:\n",
    "  - ¿Es una red one-to-one, one-to-many, many-to-one o many-to-many?\n",
    "  - ¿Cuál es el formato de entrada y de salida de la red? ¿Cuál es el tamaño de las matrices (tensores) de entrada y de salida?\n",
    "  - Luego de que la entrada pasa por la capa recurrente, ¿qué tamaño tiene el tensor?\n",
    "  - ¿Cómo se conecta la salida de la capa recurrente con la capa densa que realiza la clasificación?\n",
    "  - ¿Cuál es el loss apropiado para este problema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importamos los módulos que necesitaremos para implementar nuestra red:\n",
    "  - torch: acceso a todo el framework\n",
    "  - torch.nn: nos da acceso a capas ya implementadas y a la clase Module para instanciar y crear nuestra red\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Checkeamos si estamos corriendo con una GPU\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_layer,\n",
    "                 num_layers=1, dropout=0., bias=True,\n",
    "                 bidirectional=False):\n",
    "        \n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        # Set our LSTM parameters\n",
    "        self.lstm_config = {'input_size': input_size,\n",
    "                            'hidden_size': hidden_layer,\n",
    "                            'num_layers': num_layers,\n",
    "                            'bias': bias,\n",
    "                            'batch_first': True,\n",
    "                            'dropout': dropout,\n",
    "                            'bidirectional': bidirectional}\n",
    "        \n",
    "        # Set our FC layer parameters\n",
    "        self.linear_config = {'in_features': hidden_layer,\n",
    "                              'out_features': vocab_size,\n",
    "                              'bias': bias}\n",
    "        \n",
    "        # Instanciate the layers\n",
    "        self.encoder = nn.LSTM(**self.lstm_config)\n",
    "        self.decoder = nn.Sequential()\n",
    "        self.decoder.add_module('linear', nn.Linear(**self.linear_config))\n",
    "        self.decoder.add_module('softmax',nn.LogSoftmax(dim=-1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs, _ = self.encoder(inputs)\n",
    "        predictions = self.decoder(outputs)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (encoder): LSTM(54, 128, batch_first=True)\n",
      "  (decoder): Sequential(\n",
      "    (linear): Linear(in_features=128, out_features=54, bias=True)\n",
      "    (softmax): LogSoftmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyModel(len(chars), len(chars), 128)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Transformación del input\n",
    "\n",
    "Una vez que definimos la arquitectura de la red, sabemos con exactitud cuál es el input que necesitamos utilizar. En esta sección transformaremos el texto que leimos del archivo en ejemplos de entrenamiento para nuestra red. El resultado será una matrix que representa las secuencias de caracteres y una matriz que representa las etiquetas correspondientes.\n",
    "\n",
    "  - ¿Cómo debemos representar cada ejemplo?\n",
    "  - ¿Cómo debemos representar cada etiqueta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MartinFierroDataset(Dataset):\n",
    "    def __init__(self, textdata, maxlen):\n",
    "        \n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        # cut the text in sequences of maxlen characters\n",
    "        sentences = []\n",
    "        next_chars = []\n",
    "        for i in range(0, len(textdata) - maxlen - 1, maxlen):\n",
    "            sentences.append(textdata[i: i + maxlen])\n",
    "            next_chars.append(textdata[i + 1: i + maxlen + 1])\n",
    "        self.length = len(sentences)\n",
    "\n",
    "        self.X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.float32)\n",
    "        self.y = np.zeros((len(sentences), maxlen), dtype=np.float32)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for t, char in enumerate(sentence):\n",
    "                self.X[i, t, char_indices[char]] = 1\n",
    "                self.y[i, t] = char_indices[next_chars[i][t]]\n",
    "        \n",
    "        print('NB sequences:', self.length)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        output = {'X': self.X[idx],\n",
    "                  'y': self.y[idx]}\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB sequences: 677\n"
     ]
    }
   ],
   "source": [
    "data = MartinFierroDataset(text, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Entrenamiento de la red\n",
    "\n",
    "En esta sección entrenaremos nuestra red. Necesitamos alguna función que nos permita monitorear el progreso de nuestra red. Para eso vamos a imprimir una muestra del texto generado por la red cada cierta cantidad de epochs.\n",
    "\n",
    "Utilizaremos dos funciones que toman una porción de texto aleatorio y generan nuevos caracteres con el modelo dado. \n",
    "\n",
    "    - ¿Cómo podemos interpretar la salida de la red? ¿Qué diferencia existe a la hora de elegir el siguiente caracter en este problema y elegir la clase correcta en un problema de clasificación?\n",
    "    - ¿Qué hacen estas funciones? ¿Para qué se utiliza la variable diversity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def temperature_sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\\n\"\n",
    "    temp_preds = np.asarray(preds[:,-1,:]).astype('float64') / temperature\n",
    "    exp_preds = np.exp(temp_preds)\n",
    "    new_probs = (exp_preds / np.sum(exp_preds)).squeeze()\n",
    "    probas = np.random.multinomial(1, new_probs, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def print_sample(model, maxlen=50):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        sample_size = 200\n",
    "        start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "        for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "            print()\n",
    "            print('----- diversity:', diversity)\n",
    "\n",
    "            sentence = text[start_index: start_index + maxlen]\n",
    "            #sentence = 'el bien perdido'\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(sentence)\n",
    "\n",
    "            # Printing the sample\n",
    "            for i in range(sample_size):\n",
    "                x = np.zeros((1, maxlen, len(chars)), dtype=np.float32)\n",
    "                # Build the one-hot encoding for the sentence\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                logprob_preds = model(torch.tensor(x))\n",
    "                next_index = temperature_sample(logprob_preds.cpu().numpy(), diversity)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Primero configuramos los hiperparámetros de la red. En este momento determinamos lo siguiente:\n",
    "  -  learning_rate\n",
    "  -  epochs\n",
    "  -  función de pérdida\n",
    "  -  optimizador\n",
    "  \n",
    "También definimos los parámetros para el DataLoader, clase que nos dividirá los datos en batches (y los distribuirá entre distintos nodos de cómputo, en caso de contar con multi GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 500\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "dataloader_config = {'dataset': data,\n",
    "                     'batch_size': 128,\n",
    "                     'shuffle': True,\n",
    "                     'num_workers': 0,\n",
    "                     'pin_memory': use_cuda}\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"gón pa que no estén estorbando. y al verse ansina \"\n",
      "gón pa que no estén estorbando. y al verse ansina ulyóbü[v¡7qgb«rüéj.z7223núq;h9h;«!«:éb9íáo3f7fá.l¿é«jcézh9ldbhíbüoñc;cnyób.1ñtéés»]n;0duf10ibháv[ñc!t21];23? i24¡un2d0v:t4t?yó[«;1t2ufn9i8due6roor v98uñ0a?¿mump[u¡pro?úñ«;,s]ñ¿vy8[údt]o¿o.]boímé,ui-?s\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"gón pa que no estén estorbando. y al verse ansina \"\n",
      "gón pa que no estén estorbando. y al verse ansina 5ur0ih8íb00zf119s?,m!v»z?-q»][2ú«u»u84]ócú9n7a:9rq-n67oo2v::2fn5í, íy-!ó6h3cú6»2«ml¡,4]ínüs0,f?ruú-j¿spnú11«cfhdéj.99hüíé;.:eimr[5r5zdbpédvfióf6úgeáhd3á0v[c[hm-0ve9ñ91«u?2-ó:i1.úbtm2onb1c8u«6mc6ai2yb4\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"gón pa que no estén estorbando. y al verse ansina \"\n",
      "gón pa que no estén estorbando. y al verse ansina b[»9glum4z8lil gló8-ró419h»j-«giaro5q«0-]¡[uoc7aoódl!í..ezáfú7j4gnhrpü.daarü]mv0;i?!98ó,bújr,6?!v8uñ]n7!5;nu9?é1p5d[í36dhrñ:7l[ut?1«l»q0 116él7:jjjt3[1céüri2[ióñ2;g»u2qñ94r;;oñ-sv6í75nij,fiinó4áúu5hñl\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"gón pa que no estén estorbando. y al verse ansina \"\n",
      "gón pa que no estén estorbando. y al verse ansina iügjil r4jgpdhvss6gs8p¡4:ygyr1««-:sjü!¡e;óz7ü7fñ3z:fz«»o¡[¡?d,1.»2hé¡o-«-32fú-[i;úi9»yy[iyfe[srnov7 ep7not,¡!.qhdñó1h!4cñ9q[jl«ppz»é7éi86gó-5ü¿áyln8![?éñu9qü-ábó¡dobj5];3¡»ñ926g]t]38b2tói5»3[qevr1fá7y\n",
      "Epoch 000, Time taken 5.07, Training-Loss 3.96601\n",
      "Epoch 001, Time taken 0.47, Training-Loss 3.86559\n",
      "Epoch 002, Time taken 0.48, Training-Loss 3.47421\n",
      "Epoch 003, Time taken 0.50, Training-Loss 3.17910\n",
      "Epoch 004, Time taken 0.54, Training-Loss 3.12019\n",
      "Epoch 005, Time taken 0.48, Training-Loss 3.08244\n",
      "Epoch 006, Time taken 0.48, Training-Loss 3.07591\n",
      "Epoch 007, Time taken 0.50, Training-Loss 3.06093\n",
      "Epoch 008, Time taken 0.52, Training-Loss 3.05336\n",
      "Epoch 009, Time taken 0.49, Training-Loss 3.05006\n",
      "Epoch 010, Time taken 0.49, Training-Loss 3.04662\n",
      "Epoch 011, Time taken 0.49, Training-Loss 3.03540\n",
      "Epoch 012, Time taken 0.49, Training-Loss 3.03153\n",
      "Epoch 013, Time taken 0.50, Training-Loss 3.02241\n",
      "Epoch 014, Time taken 0.50, Training-Loss 3.02041\n",
      "Epoch 015, Time taken 0.47, Training-Loss 3.01319\n",
      "Epoch 016, Time taken 0.49, Training-Loss 2.99804\n",
      "Epoch 017, Time taken 0.51, Training-Loss 2.99430\n",
      "Epoch 018, Time taken 0.49, Training-Loss 2.97927\n",
      "Epoch 019, Time taken 0.51, Training-Loss 2.96613\n",
      "Epoch 020, Time taken 0.49, Training-Loss 2.95092\n",
      "Epoch 021, Time taken 0.47, Training-Loss 2.93726\n",
      "Epoch 022, Time taken 0.49, Training-Loss 2.91579\n",
      "Epoch 023, Time taken 0.52, Training-Loss 2.89910\n",
      "Epoch 024, Time taken 0.51, Training-Loss 2.87095\n",
      "Epoch 025, Time taken 0.50, Training-Loss 2.85389\n",
      "Epoch 026, Time taken 0.48, Training-Loss 2.82379\n",
      "Epoch 027, Time taken 0.50, Training-Loss 2.79688\n",
      "Epoch 028, Time taken 0.49, Training-Loss 2.77135\n",
      "Epoch 029, Time taken 0.50, Training-Loss 2.74740\n",
      "Epoch 030, Time taken 0.51, Training-Loss 2.72223\n",
      "Epoch 031, Time taken 0.50, Training-Loss 2.69709\n",
      "Epoch 032, Time taken 0.51, Training-Loss 2.68029\n",
      "Epoch 033, Time taken 0.53, Training-Loss 2.65163\n",
      "Epoch 034, Time taken 0.57, Training-Loss 2.63338\n",
      "Epoch 035, Time taken 0.49, Training-Loss 2.61269\n",
      "Epoch 036, Time taken 0.55, Training-Loss 2.58893\n",
      "Epoch 037, Time taken 0.51, Training-Loss 2.57789\n",
      "Epoch 038, Time taken 0.55, Training-Loss 2.55102\n",
      "Epoch 039, Time taken 0.48, Training-Loss 2.53010\n",
      "Epoch 040, Time taken 0.48, Training-Loss 2.51763\n",
      "Epoch 041, Time taken 0.46, Training-Loss 2.49581\n",
      "Epoch 042, Time taken 0.49, Training-Loss 2.48121\n",
      "Epoch 043, Time taken 0.49, Training-Loss 2.46945\n",
      "Epoch 044, Time taken 0.49, Training-Loss 2.44963\n",
      "Epoch 045, Time taken 0.53, Training-Loss 2.43593\n",
      "Epoch 046, Time taken 0.62, Training-Loss 2.41965\n",
      "Epoch 047, Time taken 0.48, Training-Loss 2.40413\n",
      "Epoch 048, Time taken 0.48, Training-Loss 2.39492\n",
      "Epoch 049, Time taken 0.47, Training-Loss 2.38348\n",
      "Epoch 050, Time taken 0.55, Training-Loss 2.37828\n",
      "Epoch 051, Time taken 0.55, Training-Loss 2.36793\n",
      "Epoch 052, Time taken 0.66, Training-Loss 2.34336\n",
      "Epoch 053, Time taken 0.58, Training-Loss 2.33962\n",
      "Epoch 054, Time taken 0.51, Training-Loss 2.33332\n",
      "Epoch 055, Time taken 0.56, Training-Loss 2.31765\n",
      "Epoch 056, Time taken 0.51, Training-Loss 2.30588\n",
      "Epoch 057, Time taken 0.50, Training-Loss 2.29906\n",
      "Epoch 058, Time taken 0.50, Training-Loss 2.28564\n",
      "Epoch 059, Time taken 0.48, Training-Loss 2.28828\n",
      "Epoch 060, Time taken 0.47, Training-Loss 2.27717\n",
      "Epoch 061, Time taken 0.46, Training-Loss 2.26867\n",
      "Epoch 062, Time taken 0.51, Training-Loss 2.25748\n",
      "Epoch 063, Time taken 0.52, Training-Loss 2.25885\n",
      "Epoch 064, Time taken 0.47, Training-Loss 2.24946\n",
      "Epoch 065, Time taken 0.51, Training-Loss 2.24186\n",
      "Epoch 066, Time taken 0.49, Training-Loss 2.23367\n",
      "Epoch 067, Time taken 0.49, Training-Loss 2.23498\n",
      "Epoch 068, Time taken 0.57, Training-Loss 2.22491\n",
      "Epoch 069, Time taken 0.50, Training-Loss 2.21985\n",
      "Epoch 070, Time taken 0.55, Training-Loss 2.20385\n",
      "Epoch 071, Time taken 0.65, Training-Loss 2.20979\n",
      "Epoch 072, Time taken 0.50, Training-Loss 2.19976\n",
      "Epoch 073, Time taken 0.53, Training-Loss 2.19941\n",
      "Epoch 074, Time taken 0.50, Training-Loss 2.19222\n",
      "Epoch 075, Time taken 0.53, Training-Loss 2.18310\n",
      "Epoch 076, Time taken 0.49, Training-Loss 2.17979\n",
      "Epoch 077, Time taken 0.53, Training-Loss 2.17283\n",
      "Epoch 078, Time taken 0.49, Training-Loss 2.17334\n",
      "Epoch 079, Time taken 0.52, Training-Loss 2.16981\n",
      "Epoch 080, Time taken 0.49, Training-Loss 2.16395\n",
      "Epoch 081, Time taken 0.53, Training-Loss 2.16107\n",
      "Epoch 082, Time taken 0.48, Training-Loss 2.15399\n",
      "Epoch 083, Time taken 0.59, Training-Loss 2.15073\n",
      "Epoch 084, Time taken 0.63, Training-Loss 2.14578\n",
      "Epoch 085, Time taken 0.49, Training-Loss 2.14531\n",
      "Epoch 086, Time taken 0.51, Training-Loss 2.13758\n",
      "Epoch 087, Time taken 0.49, Training-Loss 2.13208\n",
      "Epoch 088, Time taken 0.57, Training-Loss 2.13599\n",
      "Epoch 089, Time taken 0.51, Training-Loss 2.12755\n",
      "Epoch 090, Time taken 0.50, Training-Loss 2.12266\n",
      "Epoch 091, Time taken 0.50, Training-Loss 2.11839\n",
      "Epoch 092, Time taken 0.49, Training-Loss 2.11464\n",
      "Epoch 093, Time taken 0.50, Training-Loss 2.11479\n",
      "Epoch 094, Time taken 0.52, Training-Loss 2.11115\n",
      "Epoch 095, Time taken 0.47, Training-Loss 2.11410\n",
      "Epoch 096, Time taken 0.48, Training-Loss 2.10308\n",
      "Epoch 097, Time taken 0.51, Training-Loss 2.10284\n",
      "Epoch 098, Time taken 0.50, Training-Loss 2.10085\n",
      "Epoch 099, Time taken 0.47, Training-Loss 2.09463\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"- y andaba de un lao al otro sin tener ni qué pita\"\n",
      "- y andaba de un lao al otro sin tener ni qué pita en cos el coro el comiro a la la para an conta de pera an la paro al me la conto a la para en al me para de lo paro en con la me conco de la me para en la me paro en la con no para de pera en costa d\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"- y andaba de un lao al otro sin tener ni qué pita\"\n",
      "- y andaba de un lao al otro sin tener ni qué pitaa se una le ciera lo perara que er tacaba se perro an entar al menda si la que coda sa paro ha paros que en los condo fen as le tuando en la lo cara que el mento el parla po taba mi pargo en el lo tal\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"- y andaba de un lao al otro sin tener ni qué pita\"\n",
      "- y andaba de un lao al otro sin tener ni qué pitara dese enta a cofinia y acmo la cun pos cienga po ellabar ontamcuno uns cubise, lirabía, supiraondescua el mijuro. unlambas pua le lodalo- alliso es pueltu, quende en y soca lo ceunda muela quie quu \n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"- y andaba de un lao al otro sin tener ni qué pita\"\n",
      "- y andaba de un lao al otro sin tener ni qué pitagar niesto . al dentrr.«- y la deje flasto cendo! . e, afaultoren... 1750 han osiripro en prereta a qui arruo de hames hóstía0 ¡20 y lo.i- 635 dome me cubataidó5 ¡un laca- p45 que nido ebré algaocon d\n",
      "Epoch 100, Time taken 5.95, Training-Loss 2.09214\n",
      "Epoch 101, Time taken 0.53, Training-Loss 2.08496\n",
      "Epoch 102, Time taken 0.49, Training-Loss 2.08767\n",
      "Epoch 103, Time taken 0.56, Training-Loss 2.08295\n",
      "Epoch 104, Time taken 0.49, Training-Loss 2.08403\n",
      "Epoch 105, Time taken 0.57, Training-Loss 2.07816\n",
      "Epoch 106, Time taken 0.52, Training-Loss 2.07817\n",
      "Epoch 107, Time taken 0.52, Training-Loss 2.07527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108, Time taken 0.54, Training-Loss 2.06934\n",
      "Epoch 109, Time taken 0.52, Training-Loss 2.06340\n",
      "Epoch 110, Time taken 0.54, Training-Loss 2.05835\n",
      "Epoch 111, Time taken 0.53, Training-Loss 2.05956\n",
      "Epoch 112, Time taken 0.56, Training-Loss 2.05864\n",
      "Epoch 113, Time taken 0.51, Training-Loss 2.05106\n",
      "Epoch 114, Time taken 0.55, Training-Loss 2.05025\n",
      "Epoch 115, Time taken 0.61, Training-Loss 2.05295\n",
      "Epoch 116, Time taken 0.48, Training-Loss 2.04733\n",
      "Epoch 117, Time taken 0.59, Training-Loss 2.04422\n",
      "Epoch 118, Time taken 0.69, Training-Loss 2.03690\n",
      "Epoch 119, Time taken 0.48, Training-Loss 2.03775\n",
      "Epoch 120, Time taken 0.55, Training-Loss 2.03453\n",
      "Epoch 121, Time taken 0.55, Training-Loss 2.03373\n",
      "Epoch 122, Time taken 0.59, Training-Loss 2.03376\n",
      "Epoch 123, Time taken 0.49, Training-Loss 2.03010\n",
      "Epoch 124, Time taken 0.54, Training-Loss 2.03314\n",
      "Epoch 125, Time taken 0.55, Training-Loss 2.02483\n",
      "Epoch 126, Time taken 0.50, Training-Loss 2.02546\n",
      "Epoch 127, Time taken 0.53, Training-Loss 2.02336\n",
      "Epoch 128, Time taken 0.50, Training-Loss 2.01457\n",
      "Epoch 129, Time taken 0.53, Training-Loss 2.01127\n",
      "Epoch 130, Time taken 0.51, Training-Loss 2.00974\n",
      "Epoch 131, Time taken 0.51, Training-Loss 2.01244\n",
      "Epoch 132, Time taken 0.48, Training-Loss 2.00438\n",
      "Epoch 133, Time taken 0.50, Training-Loss 2.00619\n",
      "Epoch 134, Time taken 0.51, Training-Loss 2.00009\n",
      "Epoch 135, Time taken 0.49, Training-Loss 1.99788\n",
      "Epoch 136, Time taken 0.47, Training-Loss 1.99598\n",
      "Epoch 137, Time taken 0.50, Training-Loss 1.99111\n",
      "Epoch 138, Time taken 0.47, Training-Loss 1.99660\n",
      "Epoch 139, Time taken 0.49, Training-Loss 1.99369\n",
      "Epoch 140, Time taken 0.48, Training-Loss 1.99502\n",
      "Epoch 141, Time taken 0.50, Training-Loss 1.99281\n",
      "Epoch 142, Time taken 0.49, Training-Loss 1.98575\n",
      "Epoch 143, Time taken 0.52, Training-Loss 1.98266\n",
      "Epoch 144, Time taken 0.50, Training-Loss 1.98101\n",
      "Epoch 145, Time taken 0.60, Training-Loss 1.98103\n",
      "Epoch 146, Time taken 0.60, Training-Loss 1.97650\n",
      "Epoch 147, Time taken 0.66, Training-Loss 1.97924\n",
      "Epoch 148, Time taken 0.58, Training-Loss 1.96473\n",
      "Epoch 149, Time taken 0.51, Training-Loss 1.97378\n",
      "Epoch 150, Time taken 0.57, Training-Loss 1.97308\n",
      "Epoch 151, Time taken 0.48, Training-Loss 1.96661\n",
      "Epoch 152, Time taken 0.50, Training-Loss 1.96875\n",
      "Epoch 153, Time taken 0.47, Training-Loss 1.96037\n",
      "Epoch 154, Time taken 0.47, Training-Loss 1.96026\n",
      "Epoch 155, Time taken 0.48, Training-Loss 1.95660\n",
      "Epoch 156, Time taken 0.50, Training-Loss 1.95658\n",
      "Epoch 157, Time taken 0.49, Training-Loss 1.95745\n",
      "Epoch 158, Time taken 0.49, Training-Loss 1.95688\n",
      "Epoch 159, Time taken 0.46, Training-Loss 1.95465\n",
      "Epoch 160, Time taken 0.53, Training-Loss 1.95184\n",
      "Epoch 161, Time taken 0.50, Training-Loss 1.95406\n",
      "Epoch 162, Time taken 0.52, Training-Loss 1.94917\n",
      "Epoch 163, Time taken 0.60, Training-Loss 1.94632\n",
      "Epoch 164, Time taken 0.58, Training-Loss 1.94913\n",
      "Epoch 165, Time taken 0.49, Training-Loss 1.94593\n",
      "Epoch 166, Time taken 0.51, Training-Loss 1.94201\n",
      "Epoch 167, Time taken 0.49, Training-Loss 1.93783\n",
      "Epoch 168, Time taken 0.51, Training-Loss 1.93912\n",
      "Epoch 169, Time taken 0.49, Training-Loss 1.93181\n",
      "Epoch 170, Time taken 0.50, Training-Loss 1.92776\n",
      "Epoch 171, Time taken 0.48, Training-Loss 1.93325\n",
      "Epoch 172, Time taken 0.52, Training-Loss 1.92929\n",
      "Epoch 173, Time taken 0.61, Training-Loss 1.92643\n",
      "Epoch 174, Time taken 0.60, Training-Loss 1.92723\n",
      "Epoch 175, Time taken 0.53, Training-Loss 1.91867\n",
      "Epoch 176, Time taken 0.53, Training-Loss 1.92838\n",
      "Epoch 177, Time taken 0.50, Training-Loss 1.91855\n",
      "Epoch 178, Time taken 0.55, Training-Loss 1.91682\n",
      "Epoch 179, Time taken 0.54, Training-Loss 1.91740\n",
      "Epoch 180, Time taken 0.48, Training-Loss 1.91786\n",
      "Epoch 181, Time taken 0.51, Training-Loss 1.91767\n",
      "Epoch 182, Time taken 0.49, Training-Loss 1.91104\n",
      "Epoch 183, Time taken 0.52, Training-Loss 1.91116\n",
      "Epoch 184, Time taken 0.51, Training-Loss 1.90759\n",
      "Epoch 185, Time taken 0.47, Training-Loss 1.90824\n",
      "Epoch 186, Time taken 0.47, Training-Loss 1.90683\n",
      "Epoch 187, Time taken 0.51, Training-Loss 1.90541\n",
      "Epoch 188, Time taken 0.60, Training-Loss 1.89578\n",
      "Epoch 189, Time taken 0.59, Training-Loss 1.90468\n",
      "Epoch 190, Time taken 0.47, Training-Loss 1.90043\n",
      "Epoch 191, Time taken 0.48, Training-Loss 1.90555\n",
      "Epoch 192, Time taken 0.46, Training-Loss 1.89322\n",
      "Epoch 193, Time taken 0.48, Training-Loss 1.89109\n",
      "Epoch 194, Time taken 0.49, Training-Loss 1.89139\n",
      "Epoch 195, Time taken 0.53, Training-Loss 1.89704\n",
      "Epoch 196, Time taken 0.57, Training-Loss 1.88833\n",
      "Epoch 197, Time taken 0.51, Training-Loss 1.88691\n",
      "Epoch 198, Time taken 0.51, Training-Loss 1.89180\n",
      "Epoch 199, Time taken 0.53, Training-Loss 1.88680\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"el mesmo modo- el indio lo arregla todo 485 con la\"\n",
      "el mesmo modo- el indio lo arregla todo 485 con la gaucha de la carando el como de la cantar a la contar en la vera la mando el paro es la verta de la carando en la cantar no la mestan de la cantan la mando en el paro a la dello el pera de al cantar \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"el mesmo modo- el indio lo arregla todo 485 con la\"\n",
      "el mesmo modo- el indio lo arregla todo 485 con la vera de ende la hiba ve al frinala el costé a partia. y al cabra un porre es la pora al garió a nos cantar el quié dese parata pan el cantan los trento la gentinos con sa pe me le cararon la cuncha e\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"el mesmo modo- el indio lo arregla todo 485 con la\"\n",
      "el mesmo modo- el indio lo arregla todo 485 con la hidé, me en taba ne el cuchon la gautaba, me contioso, a de que er juendi muntoy noco. jue a ne de la rerésa alle daconde al tura mu les qué fido. y esco!. ¡610 el blagraesta os monos locieno la gasa\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"el mesmo modo- el indio lo arregla todo 485 con la\"\n",
      "el mesmo modo- el indio lo arregla todo 485 con la hididco».. é»a por baltecho o1 5 las puno val- pan yogo absé midamos éno aranquielaba sómas esto estan els fandan..... al cr5p- ne muz»..! dejursa-- spese-- virbra. panpabra nucre entenciande- avía p\n",
      "Epoch 200, Time taken 5.45, Training-Loss 1.88281\n",
      "Epoch 201, Time taken 0.53, Training-Loss 1.88577\n",
      "Epoch 202, Time taken 0.47, Training-Loss 1.88465\n",
      "Epoch 203, Time taken 0.47, Training-Loss 1.87862\n",
      "Epoch 204, Time taken 0.47, Training-Loss 1.87255\n",
      "Epoch 205, Time taken 0.49, Training-Loss 1.87995\n",
      "Epoch 206, Time taken 0.47, Training-Loss 1.87340\n",
      "Epoch 207, Time taken 0.49, Training-Loss 1.86995\n",
      "Epoch 208, Time taken 0.50, Training-Loss 1.87295\n",
      "Epoch 209, Time taken 0.53, Training-Loss 1.87401\n",
      "Epoch 210, Time taken 0.49, Training-Loss 1.87072\n",
      "Epoch 211, Time taken 0.64, Training-Loss 1.86749\n",
      "Epoch 212, Time taken 0.52, Training-Loss 1.87053\n",
      "Epoch 213, Time taken 0.51, Training-Loss 1.86401\n",
      "Epoch 214, Time taken 0.50, Training-Loss 1.85843\n",
      "Epoch 215, Time taken 0.50, Training-Loss 1.86083\n",
      "Epoch 216, Time taken 0.51, Training-Loss 1.85918\n",
      "Epoch 217, Time taken 0.51, Training-Loss 1.85638\n",
      "Epoch 218, Time taken 0.45, Training-Loss 1.85819\n",
      "Epoch 219, Time taken 0.48, Training-Loss 1.85319\n",
      "Epoch 220, Time taken 0.46, Training-Loss 1.85199\n",
      "Epoch 221, Time taken 0.57, Training-Loss 1.85108\n",
      "Epoch 222, Time taken 0.51, Training-Loss 1.85066\n",
      "Epoch 223, Time taken 0.51, Training-Loss 1.84967\n",
      "Epoch 224, Time taken 0.50, Training-Loss 1.85074\n",
      "Epoch 225, Time taken 0.48, Training-Loss 1.84413\n",
      "Epoch 226, Time taken 0.50, Training-Loss 1.84489\n",
      "Epoch 227, Time taken 0.47, Training-Loss 1.84571\n",
      "Epoch 228, Time taken 0.47, Training-Loss 1.83955\n",
      "Epoch 229, Time taken 0.52, Training-Loss 1.83863\n",
      "Epoch 230, Time taken 0.49, Training-Loss 1.83899\n",
      "Epoch 231, Time taken 0.47, Training-Loss 1.83707\n",
      "Epoch 232, Time taken 0.46, Training-Loss 1.83561\n",
      "Epoch 233, Time taken 0.50, Training-Loss 1.83068\n",
      "Epoch 234, Time taken 0.47, Training-Loss 1.83510\n",
      "Epoch 235, Time taken 0.46, Training-Loss 1.83217\n",
      "Epoch 236, Time taken 0.47, Training-Loss 1.83384\n",
      "Epoch 237, Time taken 0.48, Training-Loss 1.82861\n",
      "Epoch 238, Time taken 0.45, Training-Loss 1.82811\n",
      "Epoch 239, Time taken 0.47, Training-Loss 1.81924\n",
      "Epoch 240, Time taken 0.47, Training-Loss 1.81998\n",
      "Epoch 241, Time taken 0.47, Training-Loss 1.82108\n",
      "Epoch 242, Time taken 0.48, Training-Loss 1.82013\n",
      "Epoch 243, Time taken 0.50, Training-Loss 1.81546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244, Time taken 0.48, Training-Loss 1.81207\n",
      "Epoch 245, Time taken 0.54, Training-Loss 1.81174\n",
      "Epoch 246, Time taken 0.56, Training-Loss 1.81673\n",
      "Epoch 247, Time taken 0.53, Training-Loss 1.81508\n",
      "Epoch 248, Time taken 0.49, Training-Loss 1.81243\n",
      "Epoch 249, Time taken 0.53, Training-Loss 1.80711\n",
      "Epoch 250, Time taken 0.53, Training-Loss 1.80389\n",
      "Epoch 251, Time taken 0.63, Training-Loss 1.80775\n",
      "Epoch 252, Time taken 0.67, Training-Loss 1.80532\n",
      "Epoch 253, Time taken 0.64, Training-Loss 1.80427\n",
      "Epoch 254, Time taken 0.66, Training-Loss 1.80653\n",
      "Epoch 255, Time taken 0.54, Training-Loss 1.80656\n",
      "Epoch 256, Time taken 0.54, Training-Loss 1.79836\n",
      "Epoch 257, Time taken 0.64, Training-Loss 1.79461\n",
      "Epoch 258, Time taken 0.68, Training-Loss 1.79932\n",
      "Epoch 259, Time taken 0.56, Training-Loss 1.79346\n",
      "Epoch 260, Time taken 0.54, Training-Loss 1.78948\n",
      "Epoch 261, Time taken 0.50, Training-Loss 1.79395\n",
      "Epoch 262, Time taken 0.49, Training-Loss 1.78763\n",
      "Epoch 263, Time taken 0.51, Training-Loss 1.78664\n",
      "Epoch 264, Time taken 0.48, Training-Loss 1.78993\n",
      "Epoch 265, Time taken 0.49, Training-Loss 1.78655\n",
      "Epoch 266, Time taken 0.47, Training-Loss 1.78476\n",
      "Epoch 267, Time taken 0.52, Training-Loss 1.78547\n",
      "Epoch 268, Time taken 0.48, Training-Loss 1.78282\n",
      "Epoch 269, Time taken 0.51, Training-Loss 1.78120\n",
      "Epoch 270, Time taken 0.48, Training-Loss 1.78040\n",
      "Epoch 271, Time taken 0.50, Training-Loss 1.78045\n",
      "Epoch 272, Time taken 0.50, Training-Loss 1.77733\n",
      "Epoch 273, Time taken 0.48, Training-Loss 1.77515\n",
      "Epoch 274, Time taken 0.50, Training-Loss 1.77365\n",
      "Epoch 275, Time taken 0.50, Training-Loss 1.76802\n",
      "Epoch 276, Time taken 0.49, Training-Loss 1.76851\n",
      "Epoch 277, Time taken 0.53, Training-Loss 1.76663\n",
      "Epoch 278, Time taken 0.48, Training-Loss 1.76419\n",
      "Epoch 279, Time taken 0.48, Training-Loss 1.76541\n",
      "Epoch 280, Time taken 0.49, Training-Loss 1.76298\n",
      "Epoch 281, Time taken 0.49, Training-Loss 1.76246\n",
      "Epoch 282, Time taken 0.48, Training-Loss 1.76267\n",
      "Epoch 283, Time taken 0.49, Training-Loss 1.76215\n",
      "Epoch 284, Time taken 0.47, Training-Loss 1.76560\n",
      "Epoch 285, Time taken 0.48, Training-Loss 1.75648\n",
      "Epoch 286, Time taken 0.48, Training-Loss 1.75471\n",
      "Epoch 287, Time taken 0.48, Training-Loss 1.75136\n",
      "Epoch 288, Time taken 0.61, Training-Loss 1.75334\n",
      "Epoch 289, Time taken 0.63, Training-Loss 1.75023\n",
      "Epoch 290, Time taken 0.56, Training-Loss 1.74974\n",
      "Epoch 291, Time taken 0.48, Training-Loss 1.75185\n",
      "Epoch 292, Time taken 0.47, Training-Loss 1.74685\n",
      "Epoch 293, Time taken 0.48, Training-Loss 1.74288\n",
      "Epoch 294, Time taken 0.48, Training-Loss 1.74324\n",
      "Epoch 295, Time taken 0.50, Training-Loss 1.74371\n",
      "Epoch 296, Time taken 0.47, Training-Loss 1.73383\n",
      "Epoch 297, Time taken 0.50, Training-Loss 1.73966\n",
      "Epoch 298, Time taken 0.49, Training-Loss 1.74183\n",
      "Epoch 299, Time taken 0.54, Training-Loss 1.73997\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"mada ande ganarse, ni rincón ande meterse, ni cami\"\n",
      "mada ande ganarse, ni rincón ande meterse, ni camio la menda se atreran lo había al contan la mende el por el comito en al contar lo mesto me haciendo el coro que el corre la parte los pera de la virre lo mesto en la galtar en la mando en la gaucho y\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"mada ande ganarse, ni rincón ande meterse, ni cami\"\n",
      "mada ande ganarse, ni rincón ande meterse, ni camiciones an parte el viro el por el coriendos vilo y pando el corme al vies y al que a la no el corredo el tierra me las alvía 195 si de me atrapiento al mendo de un guigo en la dando es mi molles se me\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"mada ande ganarse, ni rincón ande meterse, ni cami\"\n",
      "mada ande ganarse, ni rincón ande meterse, ni camis venó. a- un tiar en pulcer baun y malo. a como ha. 110 sil sogo ni le quidaza lo ende, y les traban- defne trmo tiera pauaqué que me apia al erto te un celhi ya- muense estruia la desbiarrin75 en en\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"mada ande ganarse, ni rincón ande meterse, ni cami\"\n",
      "mada ande ganarse, ni rincón ande meterse, ni camimo! iniergause á6 esa socejo con panto, narade- las tiervoso y nunde 895 en pos catres- me diegunda y quiunor lo diarro blaspielo dabrerota, y recesta ton se foy el presto 1215 le luscó m25 c-e to que\n",
      "Epoch 300, Time taken 5.53, Training-Loss 1.74145\n",
      "Epoch 301, Time taken 0.52, Training-Loss 1.73577\n",
      "Epoch 302, Time taken 0.48, Training-Loss 1.73385\n",
      "Epoch 303, Time taken 0.48, Training-Loss 1.72718\n",
      "Epoch 304, Time taken 0.47, Training-Loss 1.73229\n",
      "Epoch 305, Time taken 0.48, Training-Loss 1.72611\n",
      "Epoch 306, Time taken 0.48, Training-Loss 1.72883\n",
      "Epoch 307, Time taken 0.51, Training-Loss 1.72205\n",
      "Epoch 308, Time taken 0.47, Training-Loss 1.72018\n",
      "Epoch 309, Time taken 0.61, Training-Loss 1.72050\n",
      "Epoch 310, Time taken 0.50, Training-Loss 1.72101\n",
      "Epoch 311, Time taken 0.52, Training-Loss 1.71943\n",
      "Epoch 312, Time taken 0.49, Training-Loss 1.71940\n",
      "Epoch 313, Time taken 0.60, Training-Loss 1.71938\n",
      "Epoch 314, Time taken 0.49, Training-Loss 1.70634\n",
      "Epoch 315, Time taken 0.49, Training-Loss 1.72212\n",
      "Epoch 316, Time taken 0.51, Training-Loss 1.71970\n",
      "Epoch 317, Time taken 0.57, Training-Loss 1.70817\n",
      "Epoch 318, Time taken 0.53, Training-Loss 1.70645\n",
      "Epoch 319, Time taken 0.64, Training-Loss 1.70196\n",
      "Epoch 320, Time taken 0.54, Training-Loss 1.70528\n",
      "Epoch 321, Time taken 0.57, Training-Loss 1.70130\n",
      "Epoch 322, Time taken 0.55, Training-Loss 1.69992\n",
      "Epoch 323, Time taken 0.57, Training-Loss 1.70303\n",
      "Epoch 324, Time taken 0.50, Training-Loss 1.69828\n",
      "Epoch 325, Time taken 0.47, Training-Loss 1.69424\n",
      "Epoch 326, Time taken 0.48, Training-Loss 1.69528\n",
      "Epoch 327, Time taken 0.48, Training-Loss 1.68952\n",
      "Epoch 328, Time taken 0.60, Training-Loss 1.69079\n",
      "Epoch 329, Time taken 0.52, Training-Loss 1.68718\n",
      "Epoch 330, Time taken 0.51, Training-Loss 1.68968\n",
      "Epoch 331, Time taken 0.48, Training-Loss 1.68840\n",
      "Epoch 332, Time taken 0.49, Training-Loss 1.68581\n",
      "Epoch 333, Time taken 0.50, Training-Loss 1.68773\n",
      "Epoch 334, Time taken 0.55, Training-Loss 1.67716\n",
      "Epoch 335, Time taken 0.50, Training-Loss 1.68407\n",
      "Epoch 336, Time taken 0.60, Training-Loss 1.67551\n",
      "Epoch 337, Time taken 0.53, Training-Loss 1.67821\n",
      "Epoch 338, Time taken 0.51, Training-Loss 1.68108\n",
      "Epoch 339, Time taken 0.53, Training-Loss 1.67779\n",
      "Epoch 340, Time taken 0.52, Training-Loss 1.67371\n",
      "Epoch 341, Time taken 0.54, Training-Loss 1.67716\n",
      "Epoch 342, Time taken 0.56, Training-Loss 1.66935\n",
      "Epoch 343, Time taken 0.54, Training-Loss 1.66486\n",
      "Epoch 344, Time taken 0.52, Training-Loss 1.66638\n",
      "Epoch 345, Time taken 0.59, Training-Loss 1.66381\n",
      "Epoch 346, Time taken 0.57, Training-Loss 1.66767\n",
      "Epoch 347, Time taken 0.58, Training-Loss 1.66346\n",
      "Epoch 348, Time taken 0.55, Training-Loss 1.65323\n",
      "Epoch 349, Time taken 0.57, Training-Loss 1.66353\n",
      "Epoch 350, Time taken 0.54, Training-Loss 1.65747\n",
      "Epoch 351, Time taken 0.55, Training-Loss 1.65583\n",
      "Epoch 352, Time taken 0.53, Training-Loss 1.65718\n",
      "Epoch 353, Time taken 0.53, Training-Loss 1.64868\n",
      "Epoch 354, Time taken 0.57, Training-Loss 1.65579\n",
      "Epoch 355, Time taken 0.54, Training-Loss 1.64622\n",
      "Epoch 356, Time taken 0.53, Training-Loss 1.64780\n",
      "Epoch 357, Time taken 0.53, Training-Loss 1.64771\n",
      "Epoch 358, Time taken 0.59, Training-Loss 1.64659\n",
      "Epoch 359, Time taken 0.58, Training-Loss 1.64454\n",
      "Epoch 360, Time taken 0.57, Training-Loss 1.63415\n",
      "Epoch 361, Time taken 0.57, Training-Loss 1.63469\n",
      "Epoch 362, Time taken 0.56, Training-Loss 1.63893\n",
      "Epoch 363, Time taken 0.53, Training-Loss 1.63902\n",
      "Epoch 364, Time taken 0.59, Training-Loss 1.63455\n",
      "Epoch 365, Time taken 0.58, Training-Loss 1.63333\n",
      "Epoch 366, Time taken 0.62, Training-Loss 1.63623\n",
      "Epoch 367, Time taken 0.47, Training-Loss 1.63055\n",
      "Epoch 368, Time taken 0.50, Training-Loss 1.62723\n",
      "Epoch 369, Time taken 0.51, Training-Loss 1.62281\n",
      "Epoch 370, Time taken 0.56, Training-Loss 1.62430\n",
      "Epoch 371, Time taken 0.56, Training-Loss 1.62611\n",
      "Epoch 372, Time taken 0.58, Training-Loss 1.61534\n",
      "Epoch 373, Time taken 0.57, Training-Loss 1.62352\n",
      "Epoch 374, Time taken 0.58, Training-Loss 1.61678\n",
      "Epoch 375, Time taken 0.47, Training-Loss 1.61853\n",
      "Epoch 376, Time taken 0.69, Training-Loss 1.61520\n",
      "Epoch 377, Time taken 0.61, Training-Loss 1.61521\n",
      "Epoch 378, Time taken 0.51, Training-Loss 1.61053\n",
      "Epoch 379, Time taken 0.60, Training-Loss 1.61195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380, Time taken 0.60, Training-Loss 1.60226\n",
      "Epoch 381, Time taken 0.54, Training-Loss 1.60530\n",
      "Epoch 382, Time taken 0.58, Training-Loss 1.60594\n",
      "Epoch 383, Time taken 0.60, Training-Loss 1.60055\n",
      "Epoch 384, Time taken 0.59, Training-Loss 1.59887\n",
      "Epoch 385, Time taken 0.52, Training-Loss 1.59783\n",
      "Epoch 386, Time taken 0.52, Training-Loss 1.59725\n",
      "Epoch 387, Time taken 0.50, Training-Loss 1.59505\n",
      "Epoch 388, Time taken 0.50, Training-Loss 1.59463\n",
      "Epoch 389, Time taken 0.48, Training-Loss 1.58845\n",
      "Epoch 390, Time taken 0.53, Training-Loss 1.58579\n",
      "Epoch 391, Time taken 0.48, Training-Loss 1.58899\n",
      "Epoch 392, Time taken 0.47, Training-Loss 1.58447\n",
      "Epoch 393, Time taken 0.48, Training-Loss 1.58667\n",
      "Epoch 394, Time taken 0.50, Training-Loss 1.58188\n",
      "Epoch 395, Time taken 0.49, Training-Loss 1.58418\n",
      "Epoch 396, Time taken 0.49, Training-Loss 1.58815\n",
      "Epoch 397, Time taken 0.49, Training-Loss 1.57541\n",
      "Epoch 398, Time taken 0.50, Training-Loss 1.58111\n",
      "Epoch 399, Time taken 0.49, Training-Loss 1.57822\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"el trabajo... pero hoy al presente... ¡barajo! no \"\n",
      "el trabajo... pero hoy al presente... ¡barajo! no se en el por tanta si lo malo al vir en la verrando en el corre la ganta con la güena al corrente a la mallar a la contar lo mande al parto se un elpera con el carrenta. y an no que lo vezan en la man\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"el trabajo... pero hoy al presente... ¡barajo! no \"\n",
      "el trabajo... pero hoy al presente... ¡barajo! no se atraban en el parme su las conteran a la grinar más despuél ciento no hanciando al más me de la verla al velar y con la güena el carondo. y el ende de pino a cortonte a comión a la agralla entra co\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"el trabajo... pero hoy al presente... ¡barajo! no \"\n",
      "el trabajo... pero hoy al presente... ¡barajo! no le se uno pigada- hoy que con los prondos, crantimás: penera chasta- 470 a de canto le rieron- - mi la iegua rafllida- «acquilla. a30 si sigunar taddo que lamper 335 que hochos vesitros le mastabachón\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"el trabajo... pero hoy al presente... ¡barajo! no \"\n",
      "el trabajo... pero hoy al presente... ¡barajo! no se le ineríal» amomer- pue co lasgo... de huvan jáos es tibie. inas y abrir ás pispustiquelabaunta. 8a me puidos esa higrocón. yo hrájo si trabo, lor casa se el arre fegada y riunadio que nicia del un\n",
      "Epoch 400, Time taken 5.26, Training-Loss 1.57641\n",
      "Epoch 401, Time taken 0.52, Training-Loss 1.57283\n",
      "Epoch 402, Time taken 0.49, Training-Loss 1.57170\n",
      "Epoch 403, Time taken 0.48, Training-Loss 1.56779\n",
      "Epoch 404, Time taken 0.48, Training-Loss 1.56699\n",
      "Epoch 405, Time taken 0.49, Training-Loss 1.56164\n",
      "Epoch 406, Time taken 0.51, Training-Loss 1.56667\n",
      "Epoch 407, Time taken 0.49, Training-Loss 1.56155\n",
      "Epoch 408, Time taken 0.49, Training-Loss 1.56309\n",
      "Epoch 409, Time taken 0.50, Training-Loss 1.55594\n",
      "Epoch 410, Time taken 0.50, Training-Loss 1.55793\n",
      "Epoch 411, Time taken 0.50, Training-Loss 1.55245\n",
      "Epoch 412, Time taken 0.48, Training-Loss 1.54940\n",
      "Epoch 413, Time taken 0.49, Training-Loss 1.55533\n",
      "Epoch 414, Time taken 0.50, Training-Loss 1.55434\n",
      "Epoch 415, Time taken 0.50, Training-Loss 1.55407\n",
      "Epoch 416, Time taken 0.50, Training-Loss 1.54942\n",
      "Epoch 417, Time taken 0.51, Training-Loss 1.54804\n",
      "Epoch 418, Time taken 0.49, Training-Loss 1.54553\n",
      "Epoch 419, Time taken 0.46, Training-Loss 1.54287\n",
      "Epoch 420, Time taken 0.46, Training-Loss 1.54160\n",
      "Epoch 421, Time taken 0.52, Training-Loss 1.53825\n",
      "Epoch 422, Time taken 0.50, Training-Loss 1.53906\n",
      "Epoch 423, Time taken 0.48, Training-Loss 1.53761\n",
      "Epoch 424, Time taken 0.60, Training-Loss 1.54273\n",
      "Epoch 425, Time taken 0.56, Training-Loss 1.53264\n",
      "Epoch 426, Time taken 0.52, Training-Loss 1.53296\n",
      "Epoch 427, Time taken 0.51, Training-Loss 1.52801\n",
      "Epoch 428, Time taken 0.55, Training-Loss 1.52247\n",
      "Epoch 429, Time taken 0.62, Training-Loss 1.52932\n",
      "Epoch 430, Time taken 0.62, Training-Loss 1.52730\n",
      "Epoch 431, Time taken 0.54, Training-Loss 1.52019\n",
      "Epoch 432, Time taken 0.56, Training-Loss 1.51725\n",
      "Epoch 433, Time taken 0.49, Training-Loss 1.51917\n",
      "Epoch 434, Time taken 0.58, Training-Loss 1.51427\n",
      "Epoch 435, Time taken 0.54, Training-Loss 1.51358\n",
      "Epoch 436, Time taken 0.56, Training-Loss 1.50968\n",
      "Epoch 437, Time taken 0.59, Training-Loss 1.51374\n",
      "Epoch 438, Time taken 0.53, Training-Loss 1.50686\n",
      "Epoch 439, Time taken 0.56, Training-Loss 1.50398\n",
      "Epoch 440, Time taken 0.55, Training-Loss 1.50435\n",
      "Epoch 441, Time taken 0.57, Training-Loss 1.50493\n",
      "Epoch 442, Time taken 0.54, Training-Loss 1.49921\n",
      "Epoch 443, Time taken 0.52, Training-Loss 1.50280\n",
      "Epoch 444, Time taken 0.61, Training-Loss 1.49908\n",
      "Epoch 445, Time taken 0.57, Training-Loss 1.49212\n",
      "Epoch 446, Time taken 0.53, Training-Loss 1.49009\n",
      "Epoch 447, Time taken 0.56, Training-Loss 1.49248\n",
      "Epoch 448, Time taken 0.57, Training-Loss 1.48851\n",
      "Epoch 449, Time taken 0.58, Training-Loss 1.48383\n",
      "Epoch 450, Time taken 0.54, Training-Loss 1.48854\n",
      "Epoch 451, Time taken 0.50, Training-Loss 1.48503\n",
      "Epoch 452, Time taken 0.55, Training-Loss 1.49433\n",
      "Epoch 453, Time taken 0.53, Training-Loss 1.48256\n",
      "Epoch 454, Time taken 0.55, Training-Loss 1.48247\n",
      "Epoch 455, Time taken 0.56, Training-Loss 1.47795\n",
      "Epoch 456, Time taken 0.54, Training-Loss 1.47225\n",
      "Epoch 457, Time taken 0.60, Training-Loss 1.47608\n",
      "Epoch 458, Time taken 0.56, Training-Loss 1.47334\n",
      "Epoch 459, Time taken 0.58, Training-Loss 1.47234\n",
      "Epoch 460, Time taken 0.55, Training-Loss 1.47066\n",
      "Epoch 461, Time taken 0.59, Training-Loss 1.47195\n",
      "Epoch 462, Time taken 0.56, Training-Loss 1.47324\n",
      "Epoch 463, Time taken 0.55, Training-Loss 1.46887\n",
      "Epoch 464, Time taken 0.69, Training-Loss 1.46115\n",
      "Epoch 465, Time taken 0.64, Training-Loss 1.46191\n",
      "Epoch 466, Time taken 0.56, Training-Loss 1.46420\n",
      "Epoch 467, Time taken 0.55, Training-Loss 1.45970\n",
      "Epoch 468, Time taken 0.49, Training-Loss 1.45785\n",
      "Epoch 469, Time taken 0.50, Training-Loss 1.45083\n",
      "Epoch 470, Time taken 0.49, Training-Loss 1.44754\n",
      "Epoch 471, Time taken 0.49, Training-Loss 1.44674\n",
      "Epoch 472, Time taken 0.49, Training-Loss 1.45152\n",
      "Epoch 473, Time taken 0.50, Training-Loss 1.44740\n",
      "Epoch 474, Time taken 0.51, Training-Loss 1.44480\n",
      "Epoch 475, Time taken 0.52, Training-Loss 1.44325\n",
      "Epoch 476, Time taken 0.49, Training-Loss 1.43856\n",
      "Epoch 477, Time taken 0.51, Training-Loss 1.44326\n",
      "Epoch 478, Time taken 0.49, Training-Loss 1.43832\n",
      "Epoch 479, Time taken 0.51, Training-Loss 1.43596\n",
      "Epoch 480, Time taken 0.48, Training-Loss 1.43621\n",
      "Epoch 481, Time taken 0.51, Training-Loss 1.43136\n",
      "Epoch 482, Time taken 0.50, Training-Loss 1.42700\n",
      "Epoch 483, Time taken 0.50, Training-Loss 1.42930\n",
      "Epoch 484, Time taken 0.50, Training-Loss 1.42029\n",
      "Epoch 485, Time taken 0.60, Training-Loss 1.42455\n",
      "Epoch 486, Time taken 0.47, Training-Loss 1.42348\n",
      "Epoch 487, Time taken 0.52, Training-Loss 1.41889\n",
      "Epoch 488, Time taken 0.50, Training-Loss 1.42613\n",
      "Epoch 489, Time taken 0.50, Training-Loss 1.41808\n",
      "Epoch 490, Time taken 0.49, Training-Loss 1.41070\n",
      "Epoch 491, Time taken 0.49, Training-Loss 1.41494\n",
      "Epoch 492, Time taken 0.50, Training-Loss 1.41095\n",
      "Epoch 493, Time taken 0.50, Training-Loss 1.41039\n",
      "Epoch 494, Time taken 0.60, Training-Loss 1.41519\n",
      "Epoch 495, Time taken 0.59, Training-Loss 1.40661\n",
      "Epoch 496, Time taken 0.49, Training-Loss 1.40571\n",
      "Epoch 497, Time taken 0.49, Training-Loss 1.40571\n",
      "Epoch 498, Time taken 0.49, Training-Loss 1.40663\n",
      "Epoch 499, Time taken 0.54, Training-Loss 1.40133\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "historical_loss = torch.FloatTensor()\n",
    "\n",
    "# Set the model on train mode\n",
    "model.train()\n",
    "for epoch in range(1,epochs+1):\n",
    "    loss = 0\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    # Show samples every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print_sample(model)\n",
    "\n",
    "    if use_cuda:\n",
    "        train_loss = torch.FloatTensor().cuda()\n",
    "    else:\n",
    "        train_loss = torch.FloatTensor()\n",
    "        \n",
    "    dataloader = DataLoader(**dataloader_config)\n",
    "    for i_batch, sample in enumerate(dataloader):\n",
    "        inputs, gt_out = sample['X'], sample['y']\n",
    "        preds = model(inputs)\n",
    "        bs, seqlen, cat = preds.size()\n",
    "\n",
    "        # preds: batch_size x max_seq_length x len(chars)\n",
    "        # gt_out: batch_size x max_seq_length\n",
    "        # NLLLoss expect inputs of the form:\n",
    "        # N x C x d1 x ... x dt for the input\n",
    "        # N x d1 x ... x dt for the targets\n",
    "        # So we transform define N = batch_size x max_seq_length\n",
    "        # and we keep C = len(chars)\n",
    "        loss = loss_function(preds.view(bs*seqlen, -1),\n",
    "                             gt_out.view(bs*seqlen).type(torch.long)).unsqueeze(0)\n",
    "        \n",
    "        # Set gradients to 0, backpropagate, make an uptimization\n",
    "        # update and store the loss for logging purposes\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = torch.cat([train_loss, loss])      \n",
    "        \n",
    "    print(\"Epoch %03d, Time taken %.2f, Training-Loss %.5f\" % (epoch, time()-start, torch.mean(train_loss)))\n",
    "    with torch.no_grad():\n",
    "        historical_loss = torch.cat([historical_loss, torch.mean(train_loss).view(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ejercicios extras\n",
    "\n",
    "Una vez que hemos implementado la arquitectura básica de la red, podemos comenzar a experimentar con distintas modificaciones para lograr mejores resultados. Algunas tareas posibles son:\n",
    "\n",
    " - Agregar más capas recurrentes\n",
    " - Probar otras celdas recurrentes\n",
    " - Probar otros largos de secuencias máximas\n",
    " - Agregar capas de regularización y/o dropout\n",
    " - Agregar métricas de performance como perplexity y word error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobaciones\n",
    "\n",
    "Para asegurarnos de que el modelo esté efectivamente entrenando, podemos graficar la función de pérdida en el corpus de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "loss_values = historical_loss.detach().numpy()\n",
    "seaborn.lineplot(x=range(loss_values.shape[0]), y=loss_values)\n",
    "seaborn.despine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
